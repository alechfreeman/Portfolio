# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nAu3aIfp_G7vmcpi-5XRkUJQjTnVnQ0O
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import KFold
import itertools
import time
from statistics import mean, stdev
import torch.optim as optim
import numpy as np

import os
import random
import numpy as np
import torch


#CNN Baseline

class CNNBaseline(nn.Module):
  def __init__(self, input_channels, num_classes=10, input_size=(28,28)):
    super(CNNBaseline, self).__init__()
    self.conv=nn.Sequential( #sequential container that accepts input and forwards it to a chain of modules and returning output of last module
        nn.Conv2d(input_channels, 32, kernel_size=3, padding=1), #MNIST 1 * 28 * 28 ->
                             #input_channel = 1 = Grayscale image needed from MNIST (3=RGB)
                              #input_channel = 3 = CIFAR
                             # 1-> 32 channels -> (1*28*28) -> (32*28*28)
                              # OR (3*32*32) -> (32*32*32)
        nn.ReLU(), #activation
        nn.Conv2d(32, 64, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2) # Half output (pooling)
    )

    #determine the input size after pooling
    with torch.no_grad():
      dummy = torch.zeros(1, input_channels, *input_size)
      dummy_out = self.conv(dummy)
      flatten_size = dummy_out.view(1, -1).shape[1]

    # then go through fully connected layer
    self.fc = nn.Sequential(
    nn.Flatten(), #Flatten CNN output into 1D vector

        # Then pass through dense layer with 128 units then activation then final dense layer that does classification
    nn.Linear(flatten_size, 128),
    nn.ReLU(),
    nn.Linear(128, num_classes)

    )
  #how input x flows through the NN
  def forward(self, x):
      x = self.conv(x) #pass through convolutional layer
      return self.fc(x)   #then pass through fully connected layer.


class CNNEnhanced(nn.Module):
    def __init__(self, input_channels, num_classes=10, dropout=0.3, input_size=(28,28)):
        super(CNNEnhanced, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32), #batch normalization
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(dropout), #dropout
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(dropout)
        )

        with torch.no_grad():
            dummy = torch.zeros(1, input_channels, *input_size)
            dummy_out = self.conv(dummy)
            flatten_size = dummy_out.view(1, -1).shape[1]

        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flatten_size, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        return self.fc(x)

class CNNDeep(nn.Module):
    def __init__(self, input_channels, num_classes=10, dropout=0.3, input_size=(28,28)):
        super(CNNDeep, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(dropout),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(dropout)
        )

        with torch.no_grad():
            dummy = torch.zeros(1, input_channels, *input_size)
            dummy_out = self.conv(dummy)
            flatten_size = dummy_out.view(1, -1).shape[1]

        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flatten_size, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        return self.fc(x)






def set_seed(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)  # Python general hash seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class MLP(nn.Module):
  def __init__(self, input_size, hidden_layers, output_size, dropout_rate=0.0):
      super(MLP, self).__init__()
      layers = []
      last_size = input_size
      for h in hidden_layers:
        layers.append(nn.Linear(last_size, h))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        last_size = h
      layers.append(nn.Linear(last_size, output_size))
      self.model = nn.Sequential(*layers)

  def forward(self, x): # Defines the flow of the data through the NN
    x = x.view(x.size(0), -1) # Flatten the image
    return self.model(x)

#returns data loaders of
def get_kfold_loaders(dataset, k = 3, batch_size=64):
  kf = KFold(n_splits=k, shuffle=True) # Get train/test indicies to split data in train/test sets
  # Splits dataset into k folds. Each fold used once as validation while the k - 1 remaining folds form the training set.
  #splits indices into k parts
  # have an iterator kf over (train_idx, val_index) tuples
  splits = list(kf.split(dataset)) #iterator over (train_idx, val_idx)


  for fold, (train_idx, val_idx) in enumerate(splits): #Loop through each fold where train_idx and val_idx
  #are the indices of the training and validation subsets

    #create training and validation datasets for this fold using the split indices
    train_subset = torch.utils.data.Subset(dataset, train_idx)
    val_subset = torch.utils.data.Subset(dataset, val_idx)
    #splitting data.

    #use DataLoader on datasets to be able to use them for training and evaluation
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), pin_memory=True, num_workers=2)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id))


    #generator -> yields one fold at a time
    yield fold, train_loader, val_loader


def train(model, optimizer, loader, criterion, device):
  model.train() #set model to train
  for x, y in loader:
    x, y = x.to(device), y.to(device) #put x and y on GPU
    optimizer.zero_grad() #clear previous gradients from previous step
    output = model(x) #Forward pass: predict outputs
    loss = criterion(output, y)
    loss.backward() #Backpropagate  and compute gradients
    optimizer.step() # Update weights based on gradients

def evaluate (model, loader, device):
  model.eval() #set model to evaluation mode
  correct, total = 0, 0
  with torch.no_grad(): # disable gradient tracking to save mem and speed up
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            output = model(x)   #Forward pass
            _, predicted = torch.max(output, 1) #Take class with highest score
            total += y.size(0)    #Keep count of total samples
            correct += (predicted == y).sum().item() #Count the correct predictions
            #Return boolean tensor, add up number of true values, convert to int and then add to correct prediction count
  return correct / total #return accuracy

def build_model(model_type, arch_name, hidden_layers, input_size, input_channels, dropout):
    if model_type == 'MLP':
        return MLP(input_size, hidden_layers, output_size=10, dropout_rate=dropout)
    else:
        if arch_name == 'baseline':
            return CNNBaseline(input_channels=input_channels, num_classes=10, input_size=input_size)
        elif arch_name == 'enhanced':
            return CNNEnhanced(input_channels=input_channels, num_classes=10, dropout=dropout, input_size=input_size)
        else:
            return CNNDeep(input_channels=input_channels, num_classes=10, dropout=dropout, input_size=input_size)



def run_tuning(dataset_name, dataset_train, dataset_test, input_size, model_type='MLP', input_channels=1):

    if(model_type == 'MLP'):
      architectures = {
          'shallow': [128],
          'medium': [512, 256, 128],
          'deep': [1024, 512, 256, 128, 64]
      }
    else:
      architectures = {
          #'baseline': [],
          #'enhanced': [],
          'deeper': []
      }

    learning_rates = [0.01, 0.001]
    batch_sizes = [128] #,256
    optimizers = ['sgd', 'adam']
    dropout_rates = [0.0, 0.3]
    num_epochs = 3 if dataset_name == 'MNIST' else 5


    #Check using GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_classes = 10

    results = []
    final_models = []

    for arch_name, hidden_layers in architectures.items():
        best_config = None
        best_acc = 0.0
        best_std = 0.0
        best_runtime = 0.0

        for lr, batch_size, opt_name, dropout in itertools.product(learning_rates, batch_sizes, optimizers, dropout_rates):
            # skip invalid config for CNN baseline
            if model_type != 'MLP' and arch_name == 'baseline' and dropout != 0.0:
                continue

            #reduce redudant data because equivalent or slightly worse results for different batch size or learning_rate for MLP, CIFAR10
            if model_type == 'cnn' and dataset_name == 'CIFAR10' and (arch_name == 'enhanced' or 'deeper') and (batch_size == 256): #or lr == 0.01):
                 continue

            if model_type == 'MLP' and batch_size == 256:
              continue


            val_accuracies = []
            start_time = time.time()
            set_seed(42)

            for fold, train_loader, val_loader in get_kfold_loaders(dataset_train, k=3, batch_size=batch_size):
                # Build model
                model = build_model(model_type, arch_name, hidden_layers, input_size, input_channels, dropout).to(device)
                criterion = nn.CrossEntropyLoss()
                optimizer_cls = torch.optim.SGD if opt_name == 'sgd' else torch.optim.Adam
                optimizer = optimizer_cls(model.parameters(), lr=lr)

                for epoch in range(num_epochs):
                    train(model, optimizer, train_loader, criterion, device)

                acc = evaluate(model, val_loader, device)
                val_accuracies.append(acc)

            avg_val_acc = mean(val_accuracies)
            std_val_acc = stdev(val_accuracies)
            runtime = time.time() - start_time

            print(f"{arch_name.upper()} | LR={lr} | BS={batch_size} | OPT={opt_name} | DO={dropout} --> "
                  f"Acc: {avg_val_acc:.4f} ± {std_val_acc:.4f} | Time: {runtime:.1f}s")

            if avg_val_acc > best_acc:
                best_acc = avg_val_acc
                best_std = std_val_acc
                best_runtime = runtime
                best_config = {
                    'architecture': arch_name,
                    'hidden_layers': hidden_layers,
                    'learning_rate': lr,
                    'batch_size': batch_size,
                    'optimizer': opt_name,
                    'dropout': dropout
                }

        # Train final model using best_config
        final_model = build_model(model_type, arch_name, best_config['hidden_layers'], input_size, input_channels, best_config['dropout']).to(device)
        optimizer_cls = torch.optim.SGD if best_config['optimizer'] == 'sgd' else torch.optim.Adam
        optimizer = optimizer_cls(final_model.parameters(), lr=best_config['learning_rate'])

        train_loader = DataLoader(dataset_train, batch_size=best_config['batch_size'], shuffle=True)
        test_loader = DataLoader(dataset_test, batch_size=best_config['batch_size'], shuffle=False)

        for epoch in range(num_epochs):
            train(final_model, optimizer, train_loader, criterion, device)

        test_acc = evaluate(final_model, test_loader, device)

        print(f"\n{arch_name.upper()} FINAL MODEL on {dataset_name}: Test Accuracy = {test_acc:.4f}")

        best_config.update({
            'dataset': dataset_name,
            'accuracy': best_acc,
            'std_dev': best_std,
            'runtime': best_runtime,
            'test_accuracy': test_acc
        })
        results.append(best_config)
        final_models.append(final_model)

    return results, final_models



set_seed(42)

mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))
])

# Convert images to PyTorch tensors and apply normalization for the transforms applied


cifar10_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Convert images to PyTorch tensors and apply normalization for the transforms applied


mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)
mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)

cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar10_transform)
cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar10_transform)
#Load in datasets with transform applying the transformations.


# MLP
mnist_results_mlp, best_mnist_mlp = run_tuning('MNIST', mnist_train, mnist_test, input_size=784, model_type='MLP')
cifar10_results_mlp, best_cifar10_mlp = run_tuning('CIFAR10', cifar10_train, cifar10_test, input_size=3072, model_type='MLP')

# CNN
mnist_results_cnn, best_mnist_models_cnn = run_tuning('MNIST', mnist_train, mnist_test,model_type='cnn', input_channels=1, input_size=(28,28))
cifar10_results_cnn, best_cifar10_models_cnn = run_tuning('CIFAR10', cifar10_train, cifar10_test, model_type='cnn', input_channels=3, input_size=(32,32))


# ENHANCED | LR=0.01 | BS=128 | OPT=sgd | DO=0.0 --> Acc: 0.5991 ± 0.0095 | Time: 127.5s
# ENHANCED | LR=0.01 | BS=128 | OPT=sgd | DO=0.3 --> Acc: 0.5827 ± 0.0121 | Time: 126.5s
# ENHANCED | LR=0.01 | BS=128 | OPT=adam | DO=0.0 --> Acc: 0.3551 ± 0.2472 | Time: 131.5s
# ENHANCED | LR=0.01 | BS=128 | OPT=adam | DO=0.3 --> Acc: 0.4926 ± 0.1525 | Time: 127.0s

# ENHANCED FINAL MODEL on CIFAR10: Test Accuracy = 0.6412

