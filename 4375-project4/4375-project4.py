# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uCycmEtRBHmjxhVALjb7MlKV4gpqQTse
"""

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.svm import SVC
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Fetch MNIST dataset
mnist = fetch_openml('mnist_784', version=1, as_frame=False)
X, y = mnist.data, mnist.target.astype(int)
# Shuffle and split into 10k training and 10k testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=10000, test_size=10000, random_state=42, stratify=y)

kernel_configs = {
    'linear' : {},
    'poly' : {'degree': [2,3]},
    'rbf' : {'gamma': ['scale', 0.01, 0.001]}
}

C_values = [0.1, 1, 10]


#Each fold has similar class distribution as original dataset
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

results = []

#Cross-validation loop
for kernel in kernel_configs:
    params = kernel_configs[kernel]
    if kernel == 'linear':
        for C in C_values:
            model = SVC(kernel='linear', C=C)
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
            results.append(('linear', C, '-', '-', scores.mean(), scores.std()))
    elif kernel == 'poly':
        for degree in params['degree']:
            for C in C_values:
                model = SVC(kernel='poly', degree=degree, C=C)
                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                results.append(('poly', C, degree, '-', scores.mean(), scores.std()))
    elif kernel == 'rbf':
        for gamma in params['gamma']:
            for C in C_values:
                model = SVC(kernel='rbf', gamma=gamma, C=C)
                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                results.append(('rbf', C, '-', gamma, scores.mean(), scores.std()))

# Results for each hyperparamater in a table.
final_results = pd.DataFrame(results, columns=['Kernel', 'C', 'Degree', 'Gamma', 'Mean Accuracy', 'Std Accuracy'])
print(final_results.sort_values(by='Mean Accuracy', ascending=False).reset_index(drop=True))
#get best model, fit it on all of the training data, and then evaluate on the test set
best_config = final_results.sort_values(by='Mean Accuracy', ascending=False).iloc[0]
print(f"Best configuration: {best_config}")

if best_config['Kernel'] == 'linear':
    best_model = SVC(kernel='linear', C=best_config['C'])
elif best_config['Kernel'] == 'poly':
    best_model = SVC(kernel='poly', degree=best_config['Degree'], C=best_config['C'])
else:
    best_model = SVC(kernel='rbf', gamma=best_config['Gamma'], C=best_config['C'])

best_model.fit(X_train, y_train)
test_accuracy = best_model.score(X_test, y_test)
print(f"Test Accuracy: {test_accuracy}")




# Create graph for the results
fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)

for ax, kernel in zip(axes, ['linear', 'poly', 'rbf']):
    kernel_data = final_results[final_results['Kernel'] == kernel]

    if kernel == 'linear':
        labels = kernel_data['C'].astype(str)
    elif kernel == 'poly':
        labels = ['C=' + str(c) + ', deg=' + str(d) for c, d in zip(kernel_data['C'], kernel_data['Degree'])]
    else:  # rbf
        labels = ['C=' + str(c) + ', Î³=' + str(g) for c, g in zip(kernel_data['C'], kernel_data['Gamma'])]

    ax.errorbar(labels, kernel_data['Mean Accuracy'],
                yerr=kernel_data['Std Accuracy'], fmt='o', capsize=5)
    ax.set_title(f"{kernel.upper()} Kernel")
    ax.set_xlabel("Hyperparameter Setting")
    ax.set_ylabel("Mean Accuracy")
    ax.tick_params(axis='x', rotation=45)

fig.suptitle("Cross-Validation Accuracy by Kernel and Hyperparameters", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()